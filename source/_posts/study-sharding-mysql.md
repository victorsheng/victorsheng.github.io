title: study-sharding-mysql
date: 2018-03-21 13:53:45
tags:
categories:
---
# 参考
https://tech.meituan.com/dianping_order_db_sharding.html

# 水平切分
## 切分策略
###  查询切分
将ID和库的Mapping关系记录在一个单独的库中。
优点：ID和库的Mapping算法可以随意更改。
缺点：引入额外的单点。

### 范围切分
比如按照时间区间或ID区间来切分。
优点：单表大小可控，天然水平扩展。
缺点：无法解决集中写入瓶颈的问题。

### 3. Hash切分

- 按照订单号来做hash分散订单数据
	+ 均匀分散
    + 如果要查询某用户的所有订单呢？由于是根据订单号来分散数据的。他的订单分散在了多个库、多个表中。总不能去所有的库，所有的表扫描吧。这样效率很低。
- 取模切分(较多)
	+ 要扩容的时候，为了减少迁移的数据量，一般扩容是以倍数的形式增加。比如原来是8个库，扩容的时候，就要增加到16个库，再次扩容，就增加到32个库。这样迁移的数据量，就小很多了。这个问题不算很大问题，毕竟一次扩容，可以保证比较长的时间，而且使用倍数增加的方式，已经减少了数据迁移量。


为了保持性能，每张表的数据量要控制。单表可以维持在一千万-5千万行的数据。1024*一千万。哇，可以表示很多数据了。




一般采用Mod来切分，下面着重讲一下Mod的策略。

![upload successful](/images/pasted-86.png)

数据水平切分后我们希望是一劳永逸或者是易于水平扩展的，所以推荐采用mod 2^n这种一致性Hash。

以统一订单库为例，我们分库分表的方案是32*32的，即通过UserId后四位mod 32分到32个库中，同时再将UserId后四位整除32后 Mod 32将每个库分为32个表，共计分为1024张表。线上部署情况为8个集群(主从)，每个集群4个库。

为什么说这种方式是易于水平扩展的呢？我们分析如下两个场景。

场景一：数据库性能达到瓶颈
方法一
按照现有规则不变，可以直接扩展到32个数据库集群。

![upload successful](/images/pasted-87.png)


方法二
如果32个集群也无法满足需求，那么将分库分表规则调整为(32*2^n)*(32/2^n)，可以达到最多1024个集群。


# 查询需求的考虑
   思路：既然是根据订单号分散订单数据，如果需要知道某个用户所有的订单。只要我能知道了a用户的所有的订单号，那么就可以根据订单号定位到表名称了。

  思路：既然是根据用户id来分散订单数据的。那么只要知道了这个订单号是谁的(得到了用户id)，就能知道去哪个库、哪个表查询数据了。
      那怎么知道是谁的呢？建立一个索引关系表，暂且叫做订单用户关系索引表order_user_idx。咱们命名为了保持维护性，还是一看能够知道是干嘛用的。
   存储的数据包括两项：订单号、用户编号。
   这样输入订单号，可以去查询索引关系表，获取到用户编号。
   得到了用户编号，问题解决了。订单信息是根据用户编号分库分表的，可以直接定位到x库x表了。
  当创建订单的时候，就要把关系插入到表里面去了。保存关系记录时，为了减低用户等待时间，不需要实时，做成异步。加入到消息队列中去操作。




# 唯一ID方案
# 其他方案
- 事务支持：我们是将整个订单领域聚合体切分，维度一致，所以对聚合体的事务是支持的。
- 复杂查询：垂直切分后，就跟join说拜拜了；水平切分后，查询的条件一定要在切分的维度内，比如查询具体某个用户下的各位订单等；禁止不带切分的维度的查询，即使中间件可以支持这种查询，可以在内存中组装，但是这种需求往往不应该在在线库查询，或者可以通过其他方法转换到切分的维度来实现。



![upload successful](/images/pasted-88.png)

场景二：单表容量达到瓶颈（或者1024已经无法满足你）

方法：


![upload successful](/images/pasted-89.png)

假如单表都已突破200G，200*1024=200T（按照现有的订单模型算了算，大概一万千亿订单，相信这一天，嗯，指日可待！），没关系，32*(32*2^n)，这时分库规则不变，单库里的表再进行裂变，当然，在目前订单这种规则下（用userId后四位 mod）还是有极限的，因为只有四位，所以最多拆8192个表，至于为什么只取后四位，后面会有篇幅讲到。

另外一个维度是通过ShopID进行切分，规则8*8和UserID比较类似，就不再赘述，需要注意的是Shop库我们仅存储了订单主表，用来满足Shop维度的查询。

# 数据迁移

# 其他场景

## 思考一、b2b平台的订单分卖家和买家的时候，选择什么字段来分库分表呢？

上面讨论的情况是，b2c平台。订单的卖家就一个，就是平台自己。
b2b平台，上面支持开店，买家和卖家都要能够登陆看到自己的订单。
先来看看，分表使用买家id分库分表和根据卖家id分库分表，两种办法出现的问题
如果按买家id来分库分表。有卖家的商品，会有n个用户购买，他所有的订单，会分散到多个库多个表中去了，卖家查询自己的所有订单，跨库、跨表扫描，性能低下。

如果按卖家id分库分表。买家会在n个店铺下单。订单就会分散在多个库、多个表中。买家查询自己所有订单，同样要去所有的库、所有的表搜索，性能低下。

所以，无论是按照买家id切分订单表，还是按照卖家id切分订单表。两边都不讨好。

淘宝的做法是拆分买家库和卖家库，也就是两个库：买家库、卖家库。

买家库，按照用户的id来分库分表。卖家库，按照卖家的id来分库分表。

实际上是通过数据冗余解决的：一个订单，在买家库里面有，在卖家库里面也存储了一份。下订单的时候，要写两份数据。先把订单写入买家库里面去，然后通过消息中间件来同步订单数据到卖家库里面去。

买家库的订单a修改了后，要发异步消息，通知到卖家库去，更改状态。

## 思考二：那可以按订单号来分库分表吗?  
这样分库分表的话，用户有10个订单，订单不见得都在一个库、一个表里面。查询a用户的所有订单，就会变得麻烦了。尤其是要进行分页展示，分散在不同的表，甚至不同的数据库服务器，也比较耗费性能。

那么订单号里面，最好是要有分库分表信息。淘宝的是在订单号里面添加了卖家id末2位、买家id末2位。这样的好处是干嘛呢？直接定位到具体的库、具体的表去了？

怎么根据这个呢。因为分库、分表的规则，买家库是按照卖家id末尾2位数分，卖家库是按照卖家id末尾两位分。

所以，只要从订单号里面拿到了这些数字信息，就知道在哪个库，哪个表了。

这种办法，与微信的红包订单号是类似的，末尾三位数包含了库信息、表信息。 

按照这样，其实就没必要使用订单号来计算了？

如果是按照用户id的后4位数取模分散订单数据。那么订单号的生成，可以在后面加上用户id的后4位数。

那么，虽然是按照用户id来对订单表分库分表的。其实可以直接根据订单号，知道这个订单在哪个库哪个表了。

如果是b2b系统，涉及到卖家和买家。那么可以把卖家和买家的id后面4位都加进去。不过是不是订单号太长了？

## 思考三、按照订单的时间来分表如何?

一月一张表。一年一张表。用户的所有订单，会分散在不同的库、不同的表中。

按照时间分，在切分订单数据的时候，业界用得比较少。

出现如下两个问题：


1、如果需要分页查询某个用户的所有订单数据，就会出现跨库、跨表查询。效率低。

   可以做折中：限制只能查一个范围内的订单，比如一次只能查询，一年以内或者一个月以内的订单。

2、某个时间集中写入数据，出现瓶颈。如一个月一张表。这个月的订单量暴涨呢。那么写入新的订单数据都会操作这张表。造成性能低下。影响整个业务系统交易。

真正好的分表方案，尽量将写数据分散到多个表去，达到分流效果，系统的并发能力就提高了。
