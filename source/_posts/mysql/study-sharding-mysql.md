title: mysql分库分表
date: 2018-03-21 13:53:45
tags:
    - mysql
categories:
    - mysql
---
# 参考
https://tech.meituan.com/dianping_order_db_sharding.html
https://help.aliyun.com/knowledge_list/52171.html?spm=a2c4g.11186623.6.702.CJt75C
http://shardingjdbc.io/docs_cn/02-guide/sharding/
https://github.com/MyCATApache/Mycat-doc/blob/master/%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/mycat%E5%88%86%E7%89%87%E8%A7%84%E5%88%99%20.docx

# 水平切分
## 切分策略
###  查询切分
将ID和库的Mapping关系记录在一个单独的库中。
优点：ID和库的Mapping算法可以随意更改。
缺点：引入额外的单点。

### 范围切分
比如按照时间区间或ID区间来切分。
优点：单表大小可控，天然水平扩展。
缺点：无法解决集中写入瓶颈的问题。

### 3. Hash切分

- 按照订单号来做hash分散订单数据
	+ 均匀分散
    + 如果要查询某用户的所有订单呢？由于是根据订单号来分散数据的。他的订单分散在了多个库、多个表中。总不能去所有的库，所有的表扫描吧。这样效率很低。
- 取模切分(较多)
	+ 要扩容的时候，为了减少迁移的数据量，一般扩容是以倍数的形式增加。比如原来是8个库，扩容的时候，就要增加到16个库，再次扩容，就增加到32个库。这样迁移的数据量，就小很多了。这个问题不算很大问题，毕竟一次扩容，可以保证比较长的时间，而且使用倍数增加的方式，已经减少了数据迁移量。


为了保持性能，每张表的数据量要控制。单表可以维持在一千万-5千万行的数据。1024*一千万。哇，可以表示很多数据了。




一般采用Mod来切分，下面着重讲一下Mod的策略。

![upload successful](/images/pasted-86.png)

数据水平切分后我们希望是一劳永逸或者是易于水平扩展的，所以推荐采用mod 2^n这种一致性Hash。

以统一订单库为例，我们分库分表的方案是32*32的，即通过UserId后四位mod 32分到32个库中，同时再将UserId后四位整除32后 Mod 32将每个库分为32个表，共计分为1024张表。线上部署情况为8个集群(主从)，每个集群4个库。

为什么说这种方式是易于水平扩展的呢？我们分析如下两个场景。

场景一：数据库性能达到瓶颈
方法一
按照现有规则不变，可以直接扩展到32个数据库集群。

![upload successful](/images/pasted-87.png)


方法二
如果32个集群也无法满足需求，那么将分库分表规则调整为(32*2^n)*(32/2^n)，可以达到最多1024个集群。


# 查询需求的考虑
   思路：既然是根据订单号分散订单数据，如果需要知道某个用户所有的订单。只要我能知道了a用户的所有的订单号，那么就可以根据订单号定位到表名称了。

  思路：既然是根据用户id来分散订单数据的。那么只要知道了这个订单号是谁的(得到了用户id)，就能知道去哪个库、哪个表查询数据了。
      那怎么知道是谁的呢？建立一个索引关系表，暂且叫做订单用户关系索引表order_user_idx。咱们命名为了保持维护性，还是一看能够知道是干嘛用的。
   存储的数据包括两项：订单号、用户编号。
   这样输入订单号，可以去查询索引关系表，获取到用户编号。
   得到了用户编号，问题解决了。订单信息是根据用户编号分库分表的，可以直接定位到x库x表了。
  当创建订单的时候，就要把关系插入到表里面去了。保存关系记录时，为了减低用户等待时间，不需要实时，做成异步。加入到消息队列中去操作。




# 唯一ID方案
# 其他方案
- 事务支持：我们是将整个订单领域聚合体切分，维度一致，所以对聚合体的事务是支持的。
- 复杂查询：垂直切分后，就跟join说拜拜了；水平切分后，查询的条件一定要在切分的维度内，比如查询具体某个用户下的各位订单等；禁止不带切分的维度的查询，即使中间件可以支持这种查询，可以在内存中组装，但是这种需求往往不应该在在线库查询，或者可以通过其他方法转换到切分的维度来实现。



![upload successful](/images/pasted-88.png)

场景二：单表容量达到瓶颈（或者1024已经无法满足你）

方法：


![upload successful](/images/pasted-89.png)

假如单表都已突破200G，200*1024=200T（按照现有的订单模型算了算，大概一万千亿订单，相信这一天，嗯，指日可待！），没关系，32*(32*2^n)，这时分库规则不变，单库里的表再进行裂变，当然，在目前订单这种规则下（用userId后四位 mod）还是有极限的，因为只有四位，所以最多拆8192个表，至于为什么只取后四位，后面会有篇幅讲到。

另外一个维度是通过ShopID进行切分，规则8*8和UserID比较类似，就不再赘述，需要注意的是Shop库我们仅存储了订单主表，用来满足Shop维度的查询。

# 数据迁移（从单表--切换到分表的过程）
数据库拆分一般是业务发展到一定规模后的优化和重构，为了支持业务快速上线，很难一开始就分库分表，垂直拆分还好办，改改数据源就搞定了，一旦开始水平拆分，数据清洗就是个大问题，为此，我们经历了以下几个阶段。

## 第一阶段
数据库双写（事务成功以老模型为准），查询走老模型。
每日job数据对账（通过DW），并将差异补平。
通过job导历史数据。

## 第二阶段
历史数据导入完毕并且数据对账无误。
依然是数据库双写，但是事务成功与否以新模型为准，在线查询切新模型。
每日job数据对账，将差异补平。

## 第三阶段

# 其他场景

## 思考一、b2b平台的订单分卖家和买家的时候，选择什么字段来分库分表呢？

上面讨论的情况是，b2c平台。订单的卖家就一个，就是平台自己。
b2b平台，上面支持开店，买家和卖家都要能够登陆看到自己的订单。
先来看看，分表使用买家id分库分表和根据卖家id分库分表，两种办法出现的问题
如果按买家id来分库分表。有卖家的商品，会有n个用户购买，他所有的订单，会分散到多个库多个表中去了，卖家查询自己的所有订单，跨库、跨表扫描，性能低下。

如果按卖家id分库分表。买家会在n个店铺下单。订单就会分散在多个库、多个表中。买家查询自己所有订单，同样要去所有的库、所有的表搜索，性能低下。

所以，无论是按照买家id切分订单表，还是按照卖家id切分订单表。两边都不讨好。

淘宝的做法是拆分买家库和卖家库，也就是两个库：买家库、卖家库。

买家库，按照用户的id来分库分表。卖家库，按照卖家的id来分库分表。

实际上是通过数据冗余解决的：一个订单，在买家库里面有，在卖家库里面也存储了一份。下订单的时候，要写两份数据。先把订单写入买家库里面去，然后通过消息中间件来同步订单数据到卖家库里面去。

买家库的订单a修改了后，要发异步消息，通知到卖家库去，更改状态。

## 思考二：那可以按订单号来分库分表吗?  
这样分库分表的话，用户有10个订单，订单不见得都在一个库、一个表里面。查询a用户的所有订单，就会变得麻烦了。尤其是要进行分页展示，分散在不同的表，甚至不同的数据库服务器，也比较耗费性能。

那么订单号里面，最好是要有分库分表信息。淘宝的是在订单号里面添加了卖家id末2位、买家id末2位。这样的好处是干嘛呢？直接定位到具体的库、具体的表去了？

怎么根据这个呢。因为分库、分表的规则，买家库是按照卖家id末尾2位数分，卖家库是按照卖家id末尾两位分。

所以，只要从订单号里面拿到了这些数字信息，就知道在哪个库，哪个表了。

这种办法，与微信的红包订单号是类似的，末尾三位数包含了库信息、表信息。

按照这样，其实就没必要使用订单号来计算了？

如果是按照用户id的后4位数取模分散订单数据。那么订单号的生成，可以在后面加上用户id的后4位数。

那么，虽然是按照用户id来对订单表分库分表的。其实可以直接根据订单号，知道这个订单在哪个库哪个表了。

如果是b2b系统，涉及到卖家和买家。那么可以把卖家和买家的id后面4位都加进去。不过是不是订单号太长了？

## 思考三、按照订单的时间来分表如何?

一月一张表。一年一张表。用户的所有订单，会分散在不同的库、不同的表中。

按照时间分，在切分订单数据的时候，业界用得比较少。

出现如下两个问题：


1、如果需要分页查询某个用户的所有订单数据，就会出现跨库、跨表查询。效率低。

   可以做折中：限制只能查一个范围内的订单，比如一次只能查询，一年以内或者一个月以内的订单。

2、某个时间集中写入数据，出现瓶颈。如一个月一张表。这个月的订单量暴涨呢。那么写入新的订单数据都会操作这张表。造成性能低下。影响整个业务系统交易。

真正好的分表方案，尽量将写数据分散到多个表去，达到分流效果，系统的并发能力就提高了。

# 分库分表需要解决的问题
## 事务问题
方案一：使用分布式事务
优点：交由数据库管理，简单有效
缺点：性能代价高，特别是shard越来越多时
方案二：由应用程序和数据库共同控制
原理：将一个跨多个数据库的分布式事务分拆成多个仅处 于单个数据库上面的小事务，并通过应用程序来总控 各个小事务。
优点：性能上有优势
缺点：需要应用程序在事务控制上做灵活设计。如果使用 了spring的事务管理，改动起来会面临一定的困难。

## 跨节点Join的问题
只要是进行切分，跨节点Join的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的id,根据这些id发起第二次请求得到关联数据。


## 跨节点的count,order by,group by以及聚合函数问题
这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。解决方案：与解决跨节点join问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和join不同的是每个结点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。


## 数据迁移，容量规划，扩容等问题
来自淘宝综合业务平台团队，它利用对2的倍数取余具有向前兼容的特性（如对4取余得1的数对2取余也是1）来分配数据，避免了行级别的数据迁移，但是依然需要进行表级别的迁移，同时对扩容规模和分表数量都有限制。总得来说，这些方案都不是十分的理想，多多少少都存在一些缺点，这也从一个侧面反映出了Sharding扩容的难度。



## 事务
## ID问题

## 跨分片的排序分页

## 分库策略
## 分库数量
## 路由透明


## 使用框架还是自主研发

# 如何选择分片数
DRDS 中的水平拆分有两个层次：分库和分表。每个 RDS 实例上默认会创建8个物理分库，每个物理分库上可以创建一个或多个物理分表。分表数通常也被称为分片数。

一般情况下，建议单个物理分表的容量不超过500万行数据。通常可以预估1到2年的数据增长量，用估算出的总数据量除以总的物理分库数，再除以建议的最大数据量500万，即可得出每个物理分库上需要创建的物理分表数：

`物理分库上的物理分表数 = 向上取整(估算的总数据量 / (RDS 实例数 * 8) / 5,000,000)`


# 如何选择拆分键
拆分键即分库/分表字段，是在水平拆分过程中用于生成拆分规则的数据表字段。DRDS 根据拆分键的值将数据表水平拆分到每个 RDS 实例上的物理分库中。

数据表拆分的首要原则，就是要尽可能找到数据表中的数据在业务逻辑上的主体，并确定大部分（或核心的）数据库操作都是围绕这个主体的数据进行，然后可使用该主体对应的字段作为拆分键，进行分库分表。

业务逻辑上的主体，通常与业务的应用场景相关，下面的一些典型应用场景都有明确的业务逻辑主体，可用于拆分键：

- 面向用户的互联网应用，都是围绕用户维度来做各种操作，那么业务逻辑主体就是用户，可使用用户对应的字段作为拆分键；
- 侧重于卖家的电商应用，都是围绕卖家维度来进行各种操作，那么业务逻辑主体就是卖家，可使用卖家对应的字段作为拆分键；
- 游戏类的应用，是围绕玩家维度来做各种操作，那么业务逻辑主体就是玩家，可使用玩家对应的字段作为拆分键；
- 车联网方面的应用，则是基于车辆信息进行操作，那么业务逻辑主体就是车辆，可使用车辆对应的字段作为拆分键；
- 税务类的应用，主要是基于纳税人的信息来开展前台业务，那么业务逻辑主体就是纳税人，可使用纳税人对应的字段作为拆分键。
以此类推，其它类型的应用场景，大多也能找到合适的业务逻辑主体作为拆分键的选择。

如果确实找不到合适的业务逻辑主体作为拆分键，那么可以考虑下面的方法来选择拆分键：

- 根据数据分布和访问的均衡度来考虑拆分键，尽量将数据表中的数据相对均匀地分布在不同的物理分库/分表中，适用于大量分析型查询的应用场景（查询并发度大部分能维持为1）；
- 按照数字（字符串）类型与时间类型字段相结合作为拆分键，进行分库和分表，适用于日志检索类的应用场景。

http://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/pic/29659/cn_zh/1497859380824/DRDS_overview.png
